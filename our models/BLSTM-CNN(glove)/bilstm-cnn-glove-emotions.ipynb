{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom gensim import corpora\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import SnowballStemmer\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import LSTM\nfrom sklearn import preprocessing\nfrom sklearn.metrics import (precision_score, recall_score,\n                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\nnp.random.seed(0)\nfrom tensorflow.keras.optimizers import RMSprop\nfrom keras import callbacks\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger","metadata":{"id":"e75ddd0a","execution":{"iopub.status.busy":"2021-11-14T06:27:45.899659Z","iopub.execute_input":"2021-11-14T06:27:45.899895Z","iopub.status.idle":"2021-11-14T06:27:51.853022Z","shell.execute_reply.started":"2021-11-14T06:27:45.899826Z","shell.execute_reply":"2021-11-14T06:27:51.852224Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### word2vec","metadata":{"id":"07ae1ceb"}},{"cell_type":"code","source":"from __future__ import print_function\nfrom collections import OrderedDict\n\nimport time\nimport _pickle as cPickle\n\nimport urllib\nimport matplotlib.pyplot as plt\n\nimport os\nimport sys\nimport codecs\nimport re\nimport numpy as np\n\nparameters = OrderedDict()\nparameters['word_dim'] = 300","metadata":{"id":"d10b7218","execution":{"iopub.status.busy":"2021-11-14T06:27:51.854699Z","iopub.execute_input":"2021-11-14T06:27:51.856890Z","iopub.status.idle":"2021-11-14T06:27:51.863677Z","shell.execute_reply.started":"2021-11-14T06:27:51.856843Z","shell.execute_reply":"2021-11-14T06:27:51.862991Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"traindata = pd.read_csv('../input/newsentimentdata/train.txt', sep=';', engine='python', names=['col', 'senti'])\ntestdata = pd.read_csv('../input/newsentimentdata/test.txt', sep=';', engine='python', names=['col', 'senti'])\nvaldata = pd.read_csv('../input/newsentimentdata/val.txt', sep=';', engine='python', names=['col', 'senti'])\ndatanew = pd.concat([traindata, testdata, valdata])\ndatanew.head()","metadata":{"id":"1d4efccc","outputId":"af81a79d-1ba2-42b6-974d-39a33d888ee6","execution":{"iopub.status.busy":"2021-11-14T06:27:51.865183Z","iopub.execute_input":"2021-11-14T06:27:51.868924Z","iopub.status.idle":"2021-11-14T06:27:52.040846Z","shell.execute_reply.started":"2021-11-14T06:27:51.868880Z","shell.execute_reply":"2021-11-14T06:27:52.040079Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"## encode sentiment column to onehot\nfrom sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(datanew['senti'].to_numpy().reshape(-1, 1))\nonehot_sentiment = enc.transform(datanew['senti'].to_numpy().reshape(-1, 1)).toarray()\n# onehot_sentiment = onehot_sentiment.reshape((40000, 1, 13))\ndatanew.insert(1, \"onehot_sentiment\", list(onehot_sentiment))\ndatanew","metadata":{"id":"f3b30e52","outputId":"2bde8437-9b0e-4acd-c59f-0fe1b40208a2","execution":{"iopub.status.busy":"2021-11-14T06:27:52.042961Z","iopub.execute_input":"2021-11-14T06:27:52.043590Z","iopub.status.idle":"2021-11-14T06:27:52.080614Z","shell.execute_reply.started":"2021-11-14T06:27:52.043551Z","shell.execute_reply":"2021-11-14T06:27:52.079952Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### encode word","metadata":{"id":"8df8ec3d"}},{"cell_type":"code","source":"def create_dico(item_list):\n    \"\"\"\n    Create a dictionary of items from a list of list of items.\n    (item:count) pairs\n    \"\"\"\n    assert type(item_list) is list\n    dico = {}\n    for items in item_list:\n        for item in items:\n            if item not in dico:\n                dico[item] = 1\n            else:\n                dico[item] += 1\n    return dico\n\ndef create_mapping(dico):\n    \"\"\"\n    Create a mapping (item to ID / ID to item) from a dictionary.\n    Items are ordered by decreasing frequency.\n    \"\"\"\n    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n    item_to_id = {v: k for k, v in id_to_item.items()}\n    return item_to_id, id_to_item\n\ndef word_mapping(sentences, lower):\n    \"\"\"\n    Create a dictionary and a mapping of words, sorted by frequency.\n    \n    return:\n    dico: dictionary of (word:frequency) pairs\n    word_to_id: dictionary of (word, id)\n    id_to_word: dictionary of (id, word)\n    \"\"\"\n    words = [[x.lower() if lower else x[0] for x in s] for s in sentences]\n    dico = create_dico(words)\n    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n    word_to_id, id_to_word = create_mapping(dico)\n    print(\"Found %i unique words (%i in total)\" % (\n        len(dico), sum(len(x) for x in words)\n    ))\n    return dico, word_to_id, id_to_word","metadata":{"id":"9d02a586","execution":{"iopub.status.busy":"2021-11-14T06:27:52.081818Z","iopub.execute_input":"2021-11-14T06:27:52.082078Z","iopub.status.idle":"2021-11-14T06:27:52.092445Z","shell.execute_reply.started":"2021-11-14T06:27:52.082033Z","shell.execute_reply":"2021-11-14T06:27:52.091543Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')","metadata":{"id":"aaUiKLPvNGKh","outputId":"ee0c1d79-5294-4bd7-f826-565eaa182d51","execution":{"iopub.status.busy":"2021-11-14T06:27:52.093769Z","iopub.execute_input":"2021-11-14T06:27:52.094136Z","iopub.status.idle":"2021-11-14T06:27:52.300524Z","shell.execute_reply.started":"2021-11-14T06:27:52.094099Z","shell.execute_reply":"2021-11-14T06:27:52.298937Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"## tokenising sentences\nraw_content = datanew['col'].to_numpy()\n\nprocessed_docs = []\nfor doc in raw_content:\n    tokens = word_tokenize(doc)\n#        filtered = [word for word in tokens if word not in stop_words]\n#        stemmed = [stemmer.stem(word) for word in filtered]\n    processed_docs.append(tokens)","metadata":{"id":"32536d06","execution":{"iopub.status.busy":"2021-11-14T06:27:52.302228Z","iopub.execute_input":"2021-11-14T06:27:52.302720Z","iopub.status.idle":"2021-11-14T06:27:56.418015Z","shell.execute_reply.started":"2021-11-14T06:27:52.302679Z","shell.execute_reply":"2021-11-14T06:27:56.417283Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"dico_words,word_to_id,id_to_word = word_mapping(processed_docs, True)","metadata":{"id":"a2e10b86","outputId":"159980c6-532e-4834-9fda-0f45f425092d","execution":{"iopub.status.busy":"2021-11-14T06:27:56.420229Z","iopub.execute_input":"2021-11-14T06:27:56.420738Z","iopub.status.idle":"2021-11-14T06:27:56.611220Z","shell.execute_reply.started":"2021-11-14T06:27:56.420698Z","shell.execute_reply":"2021-11-14T06:27:56.610453Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def lower_case(x,lower=False):\n    if lower:\n        return x.lower()  \n    else:\n        return x","metadata":{"id":"03254b00","execution":{"iopub.status.busy":"2021-11-14T06:27:56.612532Z","iopub.execute_input":"2021-11-14T06:27:56.612951Z","iopub.status.idle":"2021-11-14T06:27:56.617516Z","shell.execute_reply.started":"2021-11-14T06:27:56.612910Z","shell.execute_reply":"2021-11-14T06:27:56.616859Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(sentences, word_to_id, lower=False):\n    \"\"\"\n    Prepare the dataset. Return a list of lists of dictionaries containing:\n        - word indexes\n        - word char indexes\n        - tag indexes\n    \"\"\"\n    data = []\n    for s in sentences:\n        str_words = [w for w in s]\n        words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n                 for w in str_words]\n        data.append({\n            'str_words': str_words, #list of all words in the sentence\n            'words': words, #list of word index for all words in the sentence\n        })\n    return data\n\ndata = prepare_dataset(\n    processed_docs, word_to_id, True\n)\n\nprint(\"{} sentences\".format(len(data)))","metadata":{"id":"022ab1c7","outputId":"c18c9dc0-2ed0-4783-ccd1-ea1792943735","execution":{"iopub.status.busy":"2021-11-14T06:27:56.620457Z","iopub.execute_input":"2021-11-14T06:27:56.620890Z","iopub.status.idle":"2021-11-14T06:27:57.070079Z","shell.execute_reply.started":"2021-11-14T06:27:56.620851Z","shell.execute_reply":"2021-11-14T06:27:57.069329Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"all_word_embeds = {}\nfor i, line in enumerate(codecs.open('../input/newsentimentdata/glove.6B.300d.txt', 'r', 'utf-8')):\n    s = line.strip().split()\n    if len(s) == parameters['word_dim'] + 1:\n        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n\n#Intializing Word Embedding Matrix\nword_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), parameters['word_dim']))\n\nfor w in word_to_id:\n    if w in all_word_embeds:\n        word_embeds[word_to_id[w]] = all_word_embeds[w]\n    elif w.lower() in all_word_embeds:\n        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n\nprint('Loaded %i pretrained embeddings.' % len(all_word_embeds))","metadata":{"scrolled":true,"id":"7ef648e2","outputId":"1ad5eb3a-c4f2-4d46-abca-82a4e734f12a","execution":{"iopub.status.busy":"2021-11-14T06:27:57.071254Z","iopub.execute_input":"2021-11-14T06:27:57.071927Z","iopub.status.idle":"2021-11-14T06:29:03.663667Z","shell.execute_reply.started":"2021-11-14T06:27:57.071886Z","shell.execute_reply":"2021-11-14T06:29:03.662859Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"word_indices = [i['words'] for i in data]\npadded_indices = []\nfor sent in word_indices:\n    padding_size = 110 - len(sent)\n    padded = sent + [0 for i in range(padding_size)]\n    padded_indices.append(padded)","metadata":{"id":"8c8e52f8","execution":{"iopub.status.busy":"2021-11-14T06:29:03.664914Z","iopub.execute_input":"2021-11-14T06:29:03.665373Z","iopub.status.idle":"2021-11-14T06:29:03.778572Z","shell.execute_reply.started":"2021-11-14T06:29:03.665333Z","shell.execute_reply":"2021-11-14T06:29:03.777797Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"datanew.insert(1, \"padded_indices\", list(padded_indices))\ndatanew","metadata":{"id":"c79a5d1e","outputId":"68018cf7-6095-467c-afd6-56fa38549f4a","execution":{"iopub.status.busy":"2021-11-14T06:29:03.779982Z","iopub.execute_input":"2021-11-14T06:29:03.780255Z","iopub.status.idle":"2021-11-14T06:29:03.931283Z","shell.execute_reply.started":"2021-11-14T06:29:03.780220Z","shell.execute_reply":"2021-11-14T06:29:03.930583Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = datanew['padded_indices'].to_numpy()\ny = datanew['onehot_sentiment'].to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(\"X train: \", len(X_train), X_train.shape)\nprint(\"y train: \", len(y_train), y_train.shape)\nprint(\"y test: \", len(y_test), y_test.shape)","metadata":{"id":"7c70f61b","outputId":"d509f610-a628-4322-f5b8-5b0dc99392cc","execution":{"iopub.status.busy":"2021-11-14T06:29:03.932462Z","iopub.execute_input":"2021-11-14T06:29:03.933300Z","iopub.status.idle":"2021-11-14T06:29:03.946903Z","shell.execute_reply.started":"2021-11-14T06:29:03.933262Z","shell.execute_reply":"2021-11-14T06:29:03.945967Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train = np.array([np.array(x) for x in X_train])\ny_train = np.array([np.array(x) for x in y_train])\nX_test = np.array([np.array(x) for x in X_test])\ny_test = np.array([np.array(x) for x in y_test])\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","metadata":{"id":"8e9a7b3d","outputId":"924a566d-3faf-4016-c726-b4c276418b5e","execution":{"iopub.status.busy":"2021-11-14T06:29:03.948273Z","iopub.execute_input":"2021-11-14T06:29:03.948609Z","iopub.status.idle":"2021-11-14T06:29:04.451874Z","shell.execute_reply.started":"2021-11-14T06:29:03.948572Z","shell.execute_reply":"2021-11-14T06:29:04.450999Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Zhou model","metadata":{"id":"e9f90c2c"}},{"cell_type":"code","source":"# so that dont have to rerun\nimport tensorflow as tf\ntf.compat.v1.reset_default_graph()","metadata":{"id":"85bd3c1d","outputId":"5cf40201-5145-463a-845b-488dd9263c40","execution":{"iopub.status.busy":"2021-11-14T06:29:04.454061Z","iopub.execute_input":"2021-11-14T06:29:04.454589Z","iopub.status.idle":"2021-11-14T06:29:04.459073Z","shell.execute_reply.started":"2021-11-14T06:29:04.454542Z","shell.execute_reply":"2021-11-14T06:29:04.458316Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"!pip install keras_self_attention","metadata":{"execution":{"iopub.status.busy":"2021-11-14T06:29:04.460340Z","iopub.execute_input":"2021-11-14T06:29:04.460669Z","iopub.status.idle":"2021-11-14T06:29:15.123930Z","shell.execute_reply.started":"2021-11-14T06:29:04.460633Z","shell.execute_reply":"2021-11-14T06:29:15.123085Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nimport keras\nfrom keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n\n\nclass ZhouBLSTMCNNModel:\n\n    def __init__(self,\n        embedding,\n        em_drop_rate = 0.5,\n        lstm_units   = 300,\n        lstm_drop_rate = 0.5,\n        conv_size    = (3, 3),\n        conv_filters = 100,\n        pool_size    = (2, 2),\n        pool_drop_rate = 0.5):\n        '''Constructor.\n        # Parameters:\n        embedding: Numpy array representing the embedding.\n        em_drop_rate: Drop rate after the embedding layer.\n        lstm_units: Size of the internal states of the LSTM cells.\n        lstm_drop_rate: Drop rate after the lstm layer.\n        conv_size: Size of the convolutions.\n        conv_filters: Number of convolutions filters.\n        pool_size: Size for the max pooling layer.\n        pool_drop_rate: Drop rate of the max pooling layer.\n        '''\n        self._embedding      = embedding\n        self._em_drop_rate   = em_drop_rate\n        self._lstm_units     = lstm_units\n        self._lstm_drop_rate = lstm_drop_rate\n        self._conv_size      = conv_size\n        self._conv_filters   = conv_filters\n        self._pool_size      = pool_size\n        self._pool_drop_rate = pool_drop_rate\n\n    def __call__(self, input):\n        self._embedding_tf = self._create_embedding_layer(\n            self._em_drop_rate, self._embedding, input)\n\n        self._sequences_tf = self._create_blstm_layer(\n            self._lstm_units,\n            self._lstm_drop_rate,\n            self._embedding_tf)\n\n        self._convolution_tf = self._create_convolutional_layer(\n            self._conv_size,\n            self._conv_filters,\n            self._sequences_tf)\n        \n        self._pooling_tf = self._create_maxpooling_layer(\n            self._pool_size,\n            self._pool_drop_rate,\n            self._convolution_tf)\n\n        self._flatten_tf = self._create_flatten_layer(self._pooling_tf)\n\n        return self._flatten_tf\n\n    def summary(self):\n        print(\"embedding: \" + str(self._embedding_tf.shape))\n        print(\"lstm: \" + str(self._sequences_tf.shape))\n        print(\"conv: \" + str(self._convolution_tf.shape))\n        print(\"pooling: \" + str(self._pooling_tf.shape))\n        print(\"flatten: \" + str(self._flatten_tf.shape))\n\n    # this function converts each word from input_x into dense vector of dimension 300 (dimension depending on word2vec)\n    # embedding  = word_vector\n    def _create_embedding_layer(self, em_drop_rate, embedding, input_x):\n        embedding = tf.Variable(initial_value=embedding)\n\n        embedded_chars = tf.nn.embedding_lookup(  # lookup word_vector(embedding) by index (input_x)\n            embedding, tf.cast(input_x, 'int32'))\n\n        return tf.nn.dropout(embedded_chars, rate=em_drop_rate)\n\n    # bidirectional lstm layer\n    # \n    def _create_blstm_layer(self, lstm_units, lstm_drop_rate, embedding):\n        lstm_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(lstm_units)\n        sequence = tf.unstack(embedding, axis=1) #embedded input words\n        \n        # hs, _, _ are outputs, output_state_fw, output_state_bw respectively\n        hs, _, _ = tf.compat.v1.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, #forward and backward direction\n            sequence, #input\n            dtype=tf.float32)\n        \n        hs = tf.stack(\n            values=hs,\n            axis=1)\n        ss = tf.math.reduce_sum(\n            tf.reshape(hs, shape=[-1, hs.shape[1], 2, lstm_units]),\n            axis=2\n        )\n        \n        attn = SeqSelfAttention(attention_activation='sigmoid',attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL)(ss)\n\n        return tf.nn.dropout(attn, rate=lstm_drop_rate)\n\n    def _create_convolutional_layer(self,\n        conv_size, num_filters, tensor):\n        \n        print(str(tensor.shape))\n\n        filter_heigth = conv_size[0]\n        filter_width  = conv_size[1]\n\n        filter_shape = [filter_heigth, filter_width,\n            1, num_filters]\n\n        W = tf.Variable(\n            initial_value=tf.random.truncated_normal(\n                shape=filter_shape,\n                stddev=0.1))\n        b = tf.Variable(\n            initial_value=tf.random.truncated_normal(\n                shape=[num_filters]))\n\n        tensor_expanded = tf.expand_dims(tensor, -1)\n        conv = tf.nn.conv2d(\n            input=tensor_expanded,\n            filters=W, #####filter to filters, idk if it changed stuff\n            strides=[1,1,1,1],\n            padding='VALID')\n\n        bias = tf.nn.bias_add(conv, b)\n        c = tf.nn.relu(bias)\n\n        return c\n\n    def _create_maxpooling_layer(self, size, pool_drop_rate, conv):\n        pooled = tf.nn.max_pool3d(\n            input=tf.expand_dims(conv, -1),\n            ksize=[1, size[0], size[1], conv.shape[3], 1],\n            strides=[1, size[0], size[1], conv.shape[3], 1],\n            padding='VALID')\n        \n        return tf.nn.dropout(pooled, rate=pool_drop_rate)\n\n    def _create_flatten_layer(self, tensor):\n        return tf.reshape(tensor, [-1, tensor.shape[1] * tensor.shape[2]])\n\n\nif __name__ == '__main__':\n    embedding_size  = 300\n    num_words       = 1000\n    sentence_length = 10\n\n    embedding = [\n        [float(i) for i in range(embedding_size)] for _ in range(num_words)\n    ]\n    data = [\n        [i     for i in range(sentence_length)],\n        [i + 1 for i in range(sentence_length)]\n    ]\n\n    model = ZhouBLSTMCNNModel(embedding)\n    model(data)\n    model.summary()","metadata":{"id":"031cf79f","outputId":"01529036-5644-4760-a564-146576defb60","execution":{"iopub.status.busy":"2021-11-14T06:29:15.126072Z","iopub.execute_input":"2021-11-14T06:29:15.126395Z","iopub.status.idle":"2021-11-14T06:29:23.537479Z","shell.execute_reply.started":"2021-11-14T06:29:15.126353Z","shell.execute_reply":"2021-11-14T06:29:23.536691Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom numba import jit, cuda\n\nclass Classifier:\n\n    def __init__(self, model, input_length, output_length):\n        '''Constructor.\n        # Parameters:\n        input_length: sentence length (max number of words in sentence)\n        output_length: number of classes of sentiment\n        '''\n        self.model = model\n        self.input_length = input_length\n        self.output_length = output_length\n\n    def compile(self, batch_size=32):\n        tf.compat.v1.disable_eager_execution()\n        self._ds_x = tf.compat.v1.placeholder(tf.float32, [None, self.input_length])\n        self._ds_y = tf.compat.v1.placeholder(tf.float32, [None, self.output_length])\n\n        ds = tf.compat.v1.data.Dataset.from_tensor_slices((self._ds_x, self._ds_y))\n        ds = ds.batch(batch_size)\n\n        self._ds_it = ds.make_initializable_iterator()\n        self._input, self._labels = self._ds_it.get_next()\n\n        self._features = self.model(self._input)\n        self._output = _create_dense_layer(self._features, self.output_length)\n\n        self._create_acc_computations()\n        self._create_backpropagation()\n\n        self._session = tf.compat.v1.Session()\n        self._session.run(tf.compat.v1.global_variables_initializer())\n        self._session.run(tf.compat.v1.local_variables_initializer())\n\n    def _create_acc_computations(self):\n        self._predictions = tf.argmax(self._output, 1)\n        labels = tf.argmax(self._labels, 1)\n        self._accuracy = tf.reduce_mean(\n            tf.cast(tf.equal(self._predictions, labels), 'float32'))\n\n    def _create_backpropagation(self):\n        losses = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(\n            logits=self._output,\n            labels=self._labels)\n        self._loss = tf.reduce_mean(losses)\n\n        optimizer = tf.compat.v1.train.AdamOptimizer(0.001)\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n        grads_and_vars = optimizer.compute_gradients(self._loss)\n\n        self._train_op = optimizer.apply_gradients(\n            grads_and_vars, global_step=global_step)\n\n    def summary(self):\n        print('input:', self._input.shape)\n        self.model.summary()\n        print('output:', self._output.shape)\n\n#     @jit(target =\"cuda\")  \n    def train(self, X_train, y_train, X_eval, y_eval, epochs=10):\n        import time\n        start_train = time.time()\n        losslist = []\n        acclist = []\n        val_losslist = []\n        val_acclist =[]\n        for e in range(epochs):\n            start_time = time.time()\n            loss, acc = self._train(X_train, y_train)\n            losslist.append(loss)\n            acclist.append(acc)\n            duration = time.time() - start_time\n\n            val_loss, val_acc = self._eval(X_eval, y_eval)\n            val_losslist.append(val_loss)\n            val_acclist.append(val_acc)\n            \n            output = 'Epoch: {}, loss = {:.4f}, acc = {:.4f}, val_loss = {:.4f}, val_acc = {:.4f}, Time = {:.2f}s'\n            print(output.format(e + 1, loss, acc, val_loss, val_acc, duration))\n        \n        total_duration = time.time() - start_train\n        return losslist, acclist, val_losslist, val_acclist, total_duration\n\n#     @jit(target =\"cuda\")  \n    def _train(self, X_train, y_train):\n        import numpy as np\n\n        self._session.run(\n            fetches=self._ds_it.initializer,\n            feed_dict={\n                self._ds_x: X_train,\n                self._ds_y: y_train\n            })\n        loss, acc, = [], []\n        while True:\n            try:\n                _, vloss, vacc = self._session.run(\n                    fetches=[self._train_op, self._loss, self._accuracy])\n\n                loss.append(vloss)\n                acc.append(vacc)\n            except tf.errors.OutOfRangeError:\n                break\n        # endwhile\n\n        loss, acc = np.mean(loss), np.mean(acc)\n        return loss, acc\n\n    def _eval(self, X_val, y_val):\n        self._session.run(\n            fetches=self._ds_it.initializer,\n            feed_dict={\n                self._ds_x: X_val,\n                self._ds_y: y_val\n            })\n\n        loss, acc, = 0, 0\n        while True:\n            try:\n                l, vloss, vacc = self._session.run(\n                    fetches=[self._labels, self._loss, self._accuracy])\n\n                loss += vloss * len(l)\n                acc += vacc * len(l)\n            except tf.errors.OutOfRangeError:\n                break\n\n        return loss / len(X_val), acc / len(X_val)\n\n    def predict(self, X):\n        import numpy as np\n\n        self._session.run(self._ds_it.initializer,\n                         feed_dict={\n                             self._ds_x: X,\n                             self._ds_y: np.empty((len(X), self.output_length))\n                         }\n                         )\n\n        pred = list()\n        while True:\n            try:\n                ppred = self._session.run(tf.nn.softmax(self._output))\n\n                pred.extend(map(lambda l: l.tolist(), ppred))\n            except tf.errors.OutOfRangeError:\n                break\n\n        return pred\n\ndef _create_dense_layer(x, output_length):\n    '''Creates a dense layer\n    '''\n    input_size = x.shape[1] #.value\n    W = tf.Variable(\n        initial_value=tf.random.truncated_normal(\n            shape=[input_size, output_length],\n            stddev=0.1))\n    b = tf.Variable(\n        initial_value=tf.random.truncated_normal(\n            shape=[output_length]))\n\n    dense = tf.compat.v1.nn.xw_plus_b(x, W, b)\n\n    return dense\n\n\nif __name__ == '__main__':\n    pass","metadata":{"id":"b9022754","execution":{"iopub.status.busy":"2021-11-14T06:29:23.538748Z","iopub.execute_input":"2021-11-14T06:29:23.539531Z","iopub.status.idle":"2021-11-14T06:29:24.027835Z","shell.execute_reply.started":"2021-11-14T06:29:23.539489Z","shell.execute_reply":"2021-11-14T06:29:24.027119Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport sys\n# from fen.classifier import Classifier\n# from fen.pzhou_tf import ZhouBLSTMCNNModel\n\nif __name__ == '__main__':\n\n#     (X_train, y_train), (X_eval, y_eval) = (X_train, y_train), (X_test, y_test)\n\n    word_vector = word_embeds\n    word_vector = word_vector.astype('float32')\n    \n    sentence_length = X_train[0].shape[0] # sentence length = 37 -- if less than 37, remaining is 0\n    num_classes = y_train[0].shape[0] # 6 sentiment classes\n\n#     with tf.device('/device:GPU:0'):\n    model = ZhouBLSTMCNNModel(embedding=word_vector) #pass in word_vector as lookup table\n\n    classifier = Classifier(\n        model=model,\n        input_length=sentence_length,\n        output_length=num_classes)\n\n    classifier.compile(batch_size=10)\n    classifier.summary()\n    loss, acc, val_loss, val_acc, duration = classifier.train(\n        X_train=X_train,\n        y_train=y_train,\n        X_eval=X_test,\n        y_eval=y_test,\n        epochs=20\n    )\n\n    print(\"total time taken: \", duration)\n    print(\"Predictions:\", classifier.predict(X_train[0:2]))\n    print(\"Real:\", y_train[0:2])","metadata":{"id":"5d641e4d","outputId":"21842934-a521-42b1-8a69-f27197668265","execution":{"iopub.status.busy":"2021-11-14T06:29:24.029284Z","iopub.execute_input":"2021-11-14T06:29:24.029527Z","iopub.status.idle":"2021-11-14T07:21:27.687780Z","shell.execute_reply.started":"2021-11-14T06:29:24.029494Z","shell.execute_reply":"2021-11-14T07:21:27.686885Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"tokens = word_tokenize('depressing day')\ndico_words,word_to_id,id_to_word = word_mapping([tokens], True)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T07:32:20.254640Z","iopub.execute_input":"2021-11-14T07:32:20.254901Z","iopub.status.idle":"2021-11-14T07:32:20.259990Z","shell.execute_reply.started":"2021-11-14T07:32:20.254871Z","shell.execute_reply":"2021-11-14T07:32:20.259251Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"predsent1 = 'depressing day'\npredsent2 = 'i am not like everyone around me who are leading great lives and doing well at work.'\npredsent3 = 'i went to work in the morning and worked overtime and cried when i got home'\n\ndef preprocesspred(predsent):\n    tokens = word_tokenize(predsent)\n    dico_words,word_to_id,id_to_word = word_mapping([tokens], True)\n    data = prepare_dataset([tokens], word_to_id, True)\n    word_indices = [i['words'] for i in data]\n    padded_indices = []\n    for sent in word_indices:\n        padding_size = 110 - len(sent)\n        padded = sent + [0 for i in range(padding_size)]\n        padded_indices.append(padded)\n    \n    print('processed: ', padded_indices)\n    return padded_indices\n\nprocessed_predsent1 = preprocesspred(predsent1)\nprocessed_predsent2 = preprocesspred(predsent2)\nprocessed_predsent3 = preprocesspred(predsent3)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T07:48:16.874870Z","iopub.execute_input":"2021-11-14T07:48:16.875137Z","iopub.status.idle":"2021-11-14T07:48:16.887764Z","shell.execute_reply.started":"2021-11-14T07:48:16.875106Z","shell.execute_reply":"2021-11-14T07:48:16.887039Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"predvector = classifier.predict(processed_predsent1)\nprint(\"Predictions:\", predvector)\n## sadness","metadata":{"execution":{"iopub.status.busy":"2021-11-14T07:41:08.502912Z","iopub.execute_input":"2021-11-14T07:41:08.503215Z","iopub.status.idle":"2021-11-14T07:41:21.219269Z","shell.execute_reply.started":"2021-11-14T07:41:08.503158Z","shell.execute_reply":"2021-11-14T07:41:21.218351Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print(\"Predictions 2:\", classifier.predict(processed_predsent2)) ##sadness but predicted joy","metadata":{"execution":{"iopub.status.busy":"2021-11-14T07:48:40.673306Z","iopub.execute_input":"2021-11-14T07:48:40.674014Z","iopub.status.idle":"2021-11-14T07:48:44.828758Z","shell.execute_reply.started":"2021-11-14T07:48:40.673963Z","shell.execute_reply":"2021-11-14T07:48:44.827936Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"print(predsent3)\nprint(\"Predictions 3:\", classifier.predict(processed_predsent3)) ##sadness","metadata":{"execution":{"iopub.status.busy":"2021-11-14T07:45:39.016143Z","iopub.execute_input":"2021-11-14T07:45:39.016432Z","iopub.status.idle":"2021-11-14T07:45:42.505517Z","shell.execute_reply.started":"2021-11-14T07:45:39.016401Z","shell.execute_reply":"2021-11-14T07:45:42.503967Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"datanew","metadata":{"execution":{"iopub.status.busy":"2021-11-14T07:36:21.287699Z","iopub.execute_input":"2021-11-14T07:36:21.287953Z","iopub.status.idle":"2021-11-14T07:36:21.316825Z","shell.execute_reply.started":"2021-11-14T07:36:21.287924Z","shell.execute_reply":"2021-11-14T07:36:21.316060Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"epochslist = [i for i in range(0,20)]\nplt.plot(epochslist, loss)\nplt.title('training loss')\nplt.xlabel('epochs')\nplt.ylabel('training loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T09:52:41.67784Z","iopub.execute_input":"2021-11-11T09:52:41.678514Z","iopub.status.idle":"2021-11-11T09:52:41.880134Z","shell.execute_reply.started":"2021-11-11T09:52:41.678475Z","shell.execute_reply":"2021-11-11T09:52:41.87944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochslist, acc)\nplt.title('training acc')\nplt.xlabel('epochs')\nplt.ylabel('training acc')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T09:52:46.36243Z","iopub.execute_input":"2021-11-11T09:52:46.362705Z","iopub.status.idle":"2021-11-11T09:52:46.556123Z","shell.execute_reply.started":"2021-11-11T09:52:46.362674Z","shell.execute_reply":"2021-11-11T09:52:46.555399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochslist, val_loss)\nplt.title('validation loss')\nplt.xlabel('epochs')\nplt.ylabel('validation loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T09:52:50.096171Z","iopub.execute_input":"2021-11-11T09:52:50.096445Z","iopub.status.idle":"2021-11-11T09:52:50.277821Z","shell.execute_reply.started":"2021-11-11T09:52:50.096413Z","shell.execute_reply":"2021-11-11T09:52:50.277128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochslist, val_acc)\nplt.title('validation acc')\nplt.xlabel('epochs')\nplt.ylabel('validation acc')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T09:52:56.745875Z","iopub.execute_input":"2021-11-11T09:52:56.746448Z","iopub.status.idle":"2021-11-11T09:52:56.958347Z","shell.execute_reply.started":"2021-11-11T09:52:56.746409Z","shell.execute_reply":"2021-11-11T09:52:56.957587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device_name = tf.test.gpu_device_name()\nprint(device_name)","metadata":{"id":"rA2NYcIGP7dP","execution":{"iopub.status.busy":"2021-11-11T09:37:13.11105Z","iopub.status.idle":"2021-11-11T09:37:13.111512Z","shell.execute_reply.started":"2021-11-11T09:37:13.111276Z","shell.execute_reply":"2021-11-11T09:37:13.1113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### testing","metadata":{"id":"f0043b74"}},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"id":"28711b61","outputId":"f5ddbd0b-6258-4341-9b0b-650747e3449a","execution":{"iopub.status.busy":"2021-11-11T09:37:13.11262Z","iopub.status.idle":"2021-11-11T09:37:13.113406Z","shell.execute_reply.started":"2021-11-11T09:37:13.113149Z","shell.execute_reply":"2021-11-11T09:37:13.113177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = Sequential()\nmodel = (LSTM(10, input_shape=(10, 2)))\n# model.add(Dense(1))\n\n# model.summary()\nmodel.shape","metadata":{"id":"c4be507b","execution":{"iopub.status.busy":"2021-11-11T09:37:13.115096Z","iopub.status.idle":"2021-11-11T09:37:13.115534Z","shell.execute_reply.started":"2021-11-11T09:37:13.115293Z","shell.execute_reply":"2021-11-11T09:37:13.115316Z"},"trusted":true},"execution_count":null,"outputs":[]}]}